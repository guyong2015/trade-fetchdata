import json
import asyncio
from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
import html2text
import os
from datetime import datetime


def get_urls(json_file_path):
    """
    ä»æŒ‡å®šçš„JSONæ–‡ä»¶ä¸­è·å–urlsé”®å€¼
    
    Args:
        json_file_path: JSONæ–‡ä»¶çš„è·¯å¾„
        
    Returns:
        list: åŒ…å«URLä¿¡æ¯çš„åˆ—è¡¨
    """
    try:
        # è¯»å–JSONæ–‡ä»¶
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # è·å–urlsé”®å€¼
        urls = data.get('urls', [])
        
        print(f"âœ… æˆåŠŸä» {json_file_path} è·å– {len(urls)} ä¸ªURL")
        return urls
    
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {json_file_path} - {str(e)}")
        return []

async def crawl_single_webpage_to_markdown(url, source_info=""):
    """
    ä½¿ç”¨crawl4aiçˆ¬å–æŒ‡å®šç½‘é¡µçš„mainContentå…ƒç´ å¹¶è¿”å›markdownæ ¼å¼å†…å®¹
    """
    try:
        # åˆ›å»ºå¼‚æ­¥çˆ¬è™«å®ä¾‹
        async with AsyncWebCrawler(verbose=True) as crawler:
            # çˆ¬å–ç½‘é¡µ
            result = await crawler.arun(
                url=url,
                # ç­‰å¾…mainContentå…ƒç´ åŠ è½½å®Œæˆ
                wait_for_selector="#mainContent",
                timeout=30000,  # 30ç§’è¶…æ—¶
                # è®¾ç½®ç”¨æˆ·ä»£ç†
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                # ä½¿ç”¨CSSé€‰æ‹©å™¨åªæå–mainContentå†…å®¹
                css_selector="#mainContent"
            )
            
            if result.success:
                print(f"âœ… çˆ¬å–æˆåŠŸ! {source_info}")
                print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
                print(f"ğŸ”— URL: {url}")
                
                # ä»HTMLå†…å®¹ä¸­æå–mainContentå¹¶è½¬æ¢ä¸ºmarkdown
                soup = BeautifulSoup(result.html, 'html.parser')
                maincontent_element = soup.find(id='mainContent')
                
                if maincontent_element:
                    print("ğŸ” æ‰¾åˆ° mainContent å…ƒç´ ")
                    
                    # è®¾ç½®html2textè½¬æ¢å™¨
                    h = html2text.HTML2Text()
                    h.ignore_links = False
                    h.ignore_images = False
                    h.body_width = 0  # ä¸é™åˆ¶è¡Œå®½
                    
                    # è½¬æ¢mainContentä¸ºmarkdown
                    print("ğŸ“‹ å¤„ç† mainContent å…ƒç´ ...")
                    maincontent_html = str(maincontent_element)
                    markdown_content = h.handle(maincontent_html)
                    
                    print(f"ğŸ“ è½¬æ¢åå†…å®¹é•¿åº¦: {len(markdown_content)} å­—ç¬¦")
                    
                    return {
                        'success': True,
                        'source': source_info,
                        'url': url,
                        'title': result.metadata.get('title', 'N/A'),
                        'content': markdown_content,
                        'element_found': True
                    }
                else:
                    print(f"âŒ æœªæ‰¾åˆ°mainContentå…ƒç´ : {source_info}")
                    return {
                        'success': False,
                        'source': source_info,
                        'url': url,
                        'error': 'æœªæ‰¾åˆ°mainContentå…ƒç´ '
                    }
            else:
                print(f"âŒ çˆ¬å–å¤±è´¥: {source_info} - {result.error_message}")
                return {
                    'success': False,
                    'source': source_info,
                    'url': url,
                    'error': result.error_message
                }
                
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {source_info} - {str(e)}")
        return {
            'success': False,
            'source': source_info,
            'url': url,
            'error': str(e)
        }

def save_batch_results(results, output_dir, batch_num, start_index):
    """
    ä¿å­˜æ‰¹æ¬¡ç»“æœåˆ°æ–‡ä»¶
    
    Args:
        results: çˆ¬å–ç»“æœåˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        batch_num: æ‰¹æ¬¡å·
        start_index: èµ·å§‹ç´¢å¼• (å½“å‰æ‰¹æ¬¡åœ¨æ€»URLåˆ—è¡¨ä¸­çš„èµ·å§‹ç´¢å¼•)
    """
    batch_dir = os.path.join(output_dir, f"batch_{batch_num:03d}")
    os.makedirs(batch_dir, exist_ok=True)
    
    batch_content = ""
    successful_count = 0
    
    for i, result in enumerate(results):
        # æ–‡ä»¶çš„å…¨å±€ç´¢å¼•æ˜¯æ‰¹æ¬¡èµ·å§‹ç´¢å¼• + æ‰¹æ¬¡å†…ç´¢å¼• + 1
        file_index = start_index + i + 1 
        
        if result['success']:
            successful_count += 1
            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            safe_filename = f"{file_index:03d}_{result.get('name', 'unknown').replace('/', '_').replace(' ', '_')}" # Use name for filename
            file_path = os.path.join(batch_dir, f"{safe_filename}.md")
            
            # æ„å»ºæ–‡ä»¶å†…å®¹
            file_content = f"""# {result.get('name', 'Unknown')}

**æ¥æº**: {result.get('source', 'Unknown')}  
**URL**: {result['url']}  
**é¡µé¢æ ‡é¢˜**: {result['title']}  
**mainContentå…ƒç´ **: {'å·²æ‰¾åˆ°' if result.get('element_found') else 'æœªæ‰¾åˆ°'}  
**çˆ¬å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

{result['content']}
"""
            
            # ä¿å­˜å•ä¸ªæ–‡ä»¶
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(file_content)
            
            # æ·»åŠ åˆ°æ‰¹æ¬¡æ±‡æ€»å†…å®¹ä¸­
            batch_content += f"\n\n{'='*80}\n"
            batch_content += file_content
    
    # ä¿å­˜æ‰¹æ¬¡æ±‡æ€»æ–‡ä»¶
    batch_summary_file = os.path.join(batch_dir, f"batch_{batch_num:03d}_summary.md")
    with open(batch_summary_file, "w", encoding="utf-8") as f:
        f.write(f"""# æ‰¹æ¬¡ {batch_num} çˆ¬å–ç»“æœæ±‡æ€»

**æ‰¹æ¬¡å·**: {batch_num}  
**å¤„ç†èŒƒå›´**: {start_index + 1} - {start_index + len(results)}  
**çˆ¬å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**æ‰¹æ¬¡URLæ•°é‡**: {len(results)}  
**æˆåŠŸæ•°é‡**: {successful_count}  
**å¤±è´¥æ•°é‡**: {len(results) - successful_count}  

## è¯¦ç»†ç»“æœ

""")
        
        for i, result in enumerate(results):
            # è¿™é‡Œçš„ç´¢å¼•ä¹Ÿæ˜¯æ‰¹æ¬¡å†…çš„ç›¸å¯¹ç´¢å¼•
            status = "âœ… æˆåŠŸ" if result['success'] else "âŒ å¤±è´¥"
            error_msg = f" - {result.get('error', '')}" if not result['success'] else ""
            f.write(f"{start_index + i + 1}. {status} | {result.get('source', 'Unknown')} | {result.get('name', 'Unknown')}{error_msg}\n")
        
        f.write(f"\n---\n\n{batch_content}")
    
    print(f"ğŸ’¾ æ‰¹æ¬¡ {batch_num} ç»“æœå·²ä¿å­˜åˆ° {batch_dir}")
    return successful_count

def save_progress_log(output_dir, processed_count, total_count, successful_count, failed_count, start_time=None, end_time=None, output_dir_name=None):
    """
    ä¿å­˜è¿›åº¦æ—¥å¿—
    Args:
        output_dir: ç»“æœè¾“å‡ºç›®å½•çš„å®Œæ•´è·¯å¾„
        processed_count: æˆªè‡³ç›®å‰å·²å¤„ç†çš„URLæ€»æ•°
        total_count: æ€»URLæ•°é‡
        successful_count: æˆªè‡³ç›®å‰æˆåŠŸçš„URLæ€»æ•°
        failed_count: æˆªè‡³ç›®å‰å¤±è´¥çš„URLæ€»æ•°
        start_time: çˆ¬å–ä»»åŠ¡å¼€å§‹æ—¶é—´
        end_time: çˆ¬å–ä»»åŠ¡ç»“æŸæ—¶é—´ (å¦‚æœå·²å®Œæˆ)
        output_dir_name: ç»“æœè¾“å‡ºç›®å½•çš„åç§° (ä¾‹å¦‚: crawl_results_20240101_120000)
    """
    progress_file = os.path.join(output_dir, "progress_log.json")
    
    existing_data = {}
    if os.path.exists(progress_file):
        try:
            with open(progress_file, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
        except Exception:
            pass # Ignore error if file is corrupted or unreadable
    
    progress_data = {
        "start_time": start_time or existing_data.get("start_time"),
        "end_time": end_time,
        "last_update": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "processed_count": processed_count,
        "total_count": total_count,
        "successful_count": successful_count,
        "failed_count": failed_count,
        "progress_percentage": round((processed_count / total_count) * 100, 2) if total_count > 0 else 0,
        "is_completed": processed_count >= total_count,
        "output_dir_name": output_dir_name or existing_data.get("output_dir_name")
    }
    
    if progress_data["start_time"] and progress_data["end_time"]:
        try:
            start_dt = datetime.strptime(progress_data["start_time"], '%Y-%m-%d %H:%M:%S')
            end_dt = datetime.strptime(progress_data["end_time"], '%Y-%m-%d %H:%M:%S')
            duration = end_dt - start_dt
            progress_data["total_duration_seconds"] = int(duration.total_seconds())
            progress_data["total_duration_formatted"] = str(duration).split('.')[0]
        except ValueError:
            pass # Date parsing error
    
    with open(progress_file, "w", encoding="utf-8") as f:
        json.dump(progress_data, f, ensure_ascii=False, indent=2)

def load_progress_log(output_dir_prefix="crawl_results_"):
    """
    åŠ è½½æœ€æ–°çš„è¿›åº¦æ—¥å¿—ã€‚
    æŸ¥æ‰¾æœ€è¿‘åˆ›å»ºçš„ä¸”æœªå®Œæˆçš„çˆ¬å–ç›®å½•åŠå…¶è¿›åº¦æ—¥å¿—ã€‚
    """
    existing_dirs = [d for d in os.listdir('.') if os.path.isdir(d) and d.startswith(output_dir_prefix)]
    if not existing_dirs:
        return None, None

    # Sort directories by name (which includes timestamp), newest last
    existing_dirs.sort()
    
    # Iterate from newest to oldest to find an incomplete log
    for latest_output_dir in reversed(existing_dirs):
        progress_file = os.path.join(latest_output_dir, "progress_log.json")
        
        if os.path.exists(progress_file):
            try:
                with open(progress_file, "r", encoding="utf-8") as f:
                    progress_data = json.load(f)
                    if not progress_data.get("is_completed", True): # Only load if not completed
                        print(f"ğŸ“Š æ‰¾åˆ°æœªå®Œæˆçš„è¿›åº¦æ—¥å¿—: {progress_file}")
                        return progress_data, latest_output_dir
                    else:
                        print(f"âœ… è¿›åº¦æ—¥å¿—å·²å®Œæˆæˆ–æ— æ•ˆï¼Œæ£€æŸ¥ä¸‹ä¸€ä¸ªã€‚")
            except Exception as e:
                print(f"âŒ åŠ è½½è¿›åº¦æ—¥å¿—å¤±è´¥ ({progress_file}): {e}ï¼Œæ£€æŸ¥ä¸‹ä¸€ä¸ªã€‚")
    
    print(f"âœ… æœªæ‰¾åˆ°ä»»ä½•æœªå®Œæˆçš„è¿›åº¦æ—¥å¿—ï¼Œå°†å¼€å§‹æ–°çš„çˆ¬å–ã€‚")
    return None, None


def update_overall_summary(output_dir, all_results, total_successful, total_failed, current_batch, total_batches, batch_size, start_time, total_urls_overall):
    """
    æ›´æ–°æ€»ä½“æ±‡æ€»æŠ¥å‘Š (00_OVERALL_SUMMARY.md) - å®æ—¶è¿›åº¦æŠ¥å‘Š
    """
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    processed_count = len(all_results) # This now includes dummy data for resumed runs
    total_urls = total_urls_overall # Use the explicitly passed total URL count
    
    duration_info = ""
    is_completed = processed_count >= total_urls # Check if all URLs are processed based on the actual total_urls
    
    progress_file = os.path.join(output_dir, "progress_log.json")
    if os.path.exists(progress_file):
        try:
            with open(progress_file, "r", encoding="utf-8") as f:
                progress_data = json.load(f)
                if progress_data.get("total_duration_formatted"):
                    duration_info = f"**æ€»è€—æ—¶**: {progress_data['total_duration_formatted']}  \n"
                elif progress_data.get("start_time"):
                    try:
                        start_dt = datetime.strptime(progress_data["start_time"], '%Y-%m-%d %H:%M:%S')
                        current_dt = datetime.now()
                        current_duration = current_dt - start_dt
                        duration_info = f"**å½“å‰è€—æ—¶**: {str(current_duration).split('.')[0]}  \n"
                    except ValueError:
                        pass
        except Exception:
            pass
    
    summary_file = os.path.join(output_dir, "00_OVERALL_SUMMARY.md")
    
    with open(summary_file, "w", encoding="utf-8") as f:
        status_emoji = "ğŸ‰" if is_completed and processed_count == total_urls else "ğŸ”„"
        status_text = "æœ€ç»ˆçˆ¬å–æ±‡æ€»æŠ¥å‘Š" if is_completed and processed_count == total_urls else "å®æ—¶çˆ¬å–è¿›åº¦æŠ¥å‘Š"
        
        f.write(f"""# {status_emoji} {status_text}

**çˆ¬å–å¼€å§‹æ—¶é—´**: {start_time}  
**æœ€åæ›´æ–°æ—¶é—´**: {current_time}  
{duration_info}**æ€»URLæ•°é‡**: {total_urls}  
**å·²å¤„ç†URLæ•°é‡**: {processed_count}  
**å·²å®Œæˆæ‰¹æ¬¡**: {current_batch}/{total_batches}  
**æ‰¹æ¬¡å¤§å°**: {batch_size}  
**âœ… æ€»æˆåŠŸæ•°é‡**: {total_successful}  
**âŒ æ€»å¤±è´¥æ•°é‡**: {total_failed}  
**å½“å‰æˆåŠŸç‡**: {(total_successful/processed_count*100):.2f}% (åŸºäºå·²å¤„ç†çš„URL)
**æ•´ä½“è¿›åº¦**: {(processed_count/total_urls*100):.1f}%
""")
        
        if is_completed and processed_count == total_urls:
            f.write("""
## ğŸ“ æ–‡ä»¶ç»“æ„è¯´æ˜

- `00_OVERALL_SUMMARY.md` - æœ¬æ–‡ä»¶ï¼Œæ€»ä½“æ±‡æ€»æŠ¥å‘Šï¼ˆå®æ—¶æ›´æ–°ï¼‰
- `00_FINAL_SUMMARY.md` - æœ€ç»ˆæ±‡æ€»æŠ¥å‘Šï¼ˆæ¯æ‰¹æ¬¡æ›´æ–°ï¼‰
- `progress_log.json` - è¯¦ç»†è¿›åº¦æ—¥å¿—æ–‡ä»¶
""")
        else:
            f.write("""
## ğŸ“ æ–‡ä»¶ç»“æ„è¯´æ˜ (å®æ—¶æ›´æ–°)

- `00_OVERALL_SUMMARY.md` - æœ¬æ–‡ä»¶ï¼Œæ€»ä½“æ±‡æ€»æŠ¥å‘Šï¼ˆå®æ—¶æ›´æ–°ï¼‰
- `00_FINAL_SUMMARY.md` - æœ€ç»ˆæ±‡æ€»æŠ¥å‘Šï¼ˆæ¯æ‰¹æ¬¡æ›´æ–°ï¼‰
- `progress_log.json` - è¯¦ç»†è¿›åº¦æ—¥å¿—æ–‡ä»¶
""")

        f.write("\n## ğŸ“Š æ‰¹æ¬¡å¤„ç†çŠ¶æ€\n\n")
        
        for batch_num_iter in range(1, total_batches + 1):
            start_idx = (batch_num_iter - 1) * batch_size
            end_idx = min(start_idx + batch_size, total_urls)
            
            if batch_num_iter <= current_batch:
                # å·²å®Œæˆçš„æ‰¹æ¬¡
                # Note: `all_results` contains dummy results for previous batches on resume.
                # To get actual success/fail for previous batches, one might need to read `batch_XXX_summary.md`.
                # For this implementation, we rely on total_successful/total_failed and assume previous batches were successful
                # based on `progress_log.json`'s `processed_count`.
                f.write(f"- âœ… æ‰¹æ¬¡ {batch_num_iter} (URL {start_idx + 1}-{end_idx}) - å·²å®Œæˆ - `batch_{batch_num_iter:03d}/`\n")
            elif batch_num_iter == current_batch + 1:
                # æ­£åœ¨å¤„ç†çš„æ‰¹æ¬¡
                f.write(f"- ğŸ”„ æ‰¹æ¬¡ {batch_num_iter} (URL {start_idx + 1}-{end_idx}) - å¤„ç†ä¸­...\n")
            else:
                # å¾…å¤„ç†çš„æ‰¹æ¬¡
                f.write(f"- â³ æ‰¹æ¬¡ {batch_num_iter} (URL {start_idx + 1}-{end_idx}) - ç­‰å¾…å¤„ç†\n")
        
        # Display failed URLs only for the current run's failures, or all if we rebuild full all_results
        # For this batch-level resume, we show failures from the *current* run's `all_results`.
        # If `all_results` is only populated with dummy data for previous runs,
        # then this section will only show failures from the current execution segment.
        failed_results_current_run = [r for r in all_results[processed_count - (processed_count - (total_successful + total_failed)):] if not r.get('success', False)] # Slice to get only current run's results
        
        if failed_results_current_run:
            f.write(f"\n## ğŸ” å¤±è´¥URLè¯¦æƒ… (å…±{len(failed_results_current_run)}ä¸ª - å½“å‰è¿è¡Œæ‰¹æ¬¡)\n\n")
            for i, result in enumerate(failed_results_current_run, 1):
                f.write(f"{i}. **{result.get('name', 'Unknown')}**\n")
                f.write(f"   - æ¥æº: {result.get('source', 'Unknown')}\n")
                f.write(f"   - URL: {result.get('url', '')}\n")
                f.write(f"   - é”™è¯¯: {result.get('error', '')}\n\n")
        
        if is_completed and processed_count == total_urls:
            f.write(f"\n---\n\n## ğŸŠ çˆ¬å–ä»»åŠ¡å·²å…¨éƒ¨å®Œæˆï¼\n\n")
            f.write(f"æ„Ÿè°¢æ‚¨çš„è€å¿ƒç­‰å¾…ï¼Œæ‰€æœ‰ {processed_count} ä¸ªURLå·²å¤„ç†å®Œæ¯•ã€‚\n")
            f.write(f"è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹å„æ‰¹æ¬¡ç›®å½•ä¸­çš„æ–‡ä»¶ã€‚\n")
        else:
            f.write(f"\n---\n\n## â±ï¸ ä»»åŠ¡è¿›è¡Œä¸­...\n\n")
            f.write(f"å½“å‰æ­£åœ¨å¤„ç†ç¬¬ {current_batch + 1} æ‰¹æ¬¡ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚\n")
            f.write(f"æ‚¨å¯ä»¥éšæ—¶æŸ¥çœ‹æ­¤æ–‡ä»¶äº†è§£æœ€æ–°è¿›åº¦ã€‚\n")


def generate_final_summary_report(output_dir, all_results, total_successful, total_failed, url_list_original, batch_size, start_time, current_end_time, total_urls_overall):
    """
    ç”Ÿæˆæˆ–æ›´æ–°æœ€ç»ˆæ±‡æ€»æŠ¥å‘Š (00_FINAL_SUMMARY.md)
    è¿™ä¸ªæŠ¥å‘Šä¼šéšç€æ¯æ‰¹æ¬¡å¤„ç†å®Œæˆè€Œæ›´æ–°ï¼Œåæ˜ å½“å‰çš„ç´¯è®¡ç»“æœã€‚
    """
    total_urls = total_urls_overall # Use the explicitly passed total URL count
    processed_urls_count = len(all_results) # This now includes dummy data for resumed runs
    total_batches = (total_urls + batch_size - 1) // batch_size

    final_summary_file = os.path.join(output_dir, "00_FINAL_SUMMARY.md")

    progress_file = os.path.join(output_dir, "progress_log.json")
    duration_info = ""
    is_completed = processed_urls_count >= total_urls 
    
    if os.path.exists(progress_file):
        try:
            with open(progress_file, "r", encoding="utf-8") as f:
                progress_data = json.load(f)
                if progress_data.get("total_duration_formatted"):
                    duration_info = f"**æ€»è€—æ—¶**: {progress_data['total_duration_formatted']}  \n"
                elif progress_data.get("start_time") and current_end_time:
                    try:
                        start_dt = datetime.strptime(progress_data["start_time"], '%Y-%m-%d %H:%M:%S')
                        current_dt = datetime.strptime(current_end_time, '%Y-%m-%d %H:%M:%S')
                        current_duration = current_dt - start_dt
                        duration_info = f"**å½“å‰ç´¯è®¡è€—æ—¶**: {str(current_duration).split('.')[0]}  \n"
                    except ValueError:
                        pass
        except Exception:
            pass

    with open(final_summary_file, "w", encoding="utf-8") as f:
        status_emoji = "ğŸ‰" if is_completed else "ğŸ“Š"
        status_text = "æœ€ç»ˆçˆ¬å–æ±‡æ€»æŠ¥å‘Š" if is_completed else "ç´¯è®¡çˆ¬å–æ±‡æ€»æŠ¥å‘Š (å®æ—¶æ›´æ–°)"

        f.write(f"""# {status_emoji} {status_text}

**çˆ¬å–å¼€å§‹æ—¶é—´**: {start_time}  
**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {current_end_time}  
{duration_info}**æ€»URLæ•°é‡**: {total_urls}  
**å·²å¤„ç†URLæ•°é‡**: {processed_urls_count}  
**æ‰¹æ¬¡æ•°é‡**: {total_batches}  
**æ‰¹æ¬¡å¤§å°**: {batch_size}  
**âœ… æ€»æˆåŠŸæ•°é‡**: {total_successful}  
**âŒ æ€»å¤±è´¥æ•°é‡**: {total_failed}  
**æˆåŠŸç‡**: {(total_successful/processed_urls_count*100):.2f}% (åŸºäºå·²å¤„ç†çš„URL)

## ğŸ“ æ–‡ä»¶ç»“æ„è¯´æ˜
- `00_OVERALL_SUMMARY.md` - æ€»ä½“æ±‡æ€»æŠ¥å‘Šï¼ˆå®æ—¶è¿›åº¦ï¼‰
- `00_FINAL_SUMMARY.md` - æœ¬æ–‡ä»¶ï¼Œç´¯è®¡/æœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
- `progress_log.json` - è¿›åº¦æ—¥å¿—æ–‡ä»¶
- `batch_001/` - ç¬¬1æ‰¹æ¬¡ç»“æœ
- `batch_002/` - ç¬¬2æ‰¹æ¬¡ç»“æœ
- ... ä¾æ­¤ç±»æ¨

æ¯ä¸ªæ‰¹æ¬¡ç›®å½•åŒ…å«:
- `batch_XXX_summary.md` - æ‰¹æ¬¡æ±‡æ€»æ–‡ä»¶
- `001_xxx.md`, `002_xxx.md` - å•ä¸ªURLçš„çˆ¬å–ç»“æœ

## ğŸ“Š è¯¦ç»†ç»“æœç»Ÿè®¡ (åŸºäºå·²å¤„ç†çš„URL)

""")
        
        # Group results by batch for accurate statistics from all_results
        batch_wise_results = {}
        for result in all_results:
            batch_num = result.get('batch_num')
            if batch_num not in batch_wise_results:
                batch_wise_results[batch_num] = []
            batch_wise_results[batch_num].append(result)

        for batch_num in sorted(batch_wise_results.keys()):
            batch_results = batch_wise_results[batch_num]
            # Use sum(1 for r in batch_results if r.get('success', False)) if using dummy data
            # If using actual data (if rebuilt from files), then check 'success'
            batch_success = sum(1 for r in batch_results if r.get('success', False))
            batch_fail = len(batch_results) - batch_success
            
            start_idx_original = (batch_num - 1) * batch_size
            end_idx_original = min(start_idx_original + batch_size, total_urls)

            f.write(f"### æ‰¹æ¬¡ {batch_num} (URL {start_idx_original + 1}-{end_idx_original})\n")
            f.write(f"- âœ… æˆåŠŸ: {batch_success}\n")
            f.write(f"- âŒ å¤±è´¥: {batch_fail}\n")
            f.write(f"- ğŸ“ ç›®å½•: `batch_{batch_num:03d}/`\n\n")
        
        f.write("## ğŸ“‹ æ‰€æœ‰URLè¯¦ç»†å†…å®¹ (ç´¯è®¡)\n\n") 
        
        cumulative_content_for_final_report = ""
        for i, result in enumerate(all_results, 1):
            # If `all_results` contains dummy entries for previous runs,
            # this part will only show full content for the current run's processed items.
            # For previously processed (resumed) items, it will only show success/fail status
            # unless a more complex `all_results` reconstruction from files is implemented.
            if result.get('success', False) and 'content' in result: # Only show content if available
                cumulative_content_for_final_report += f"""# {result.get('name', 'Unknown')}

**æ¥æº**: {result.get('source', 'Unknown')}  
**URL**: {result['url']}  
**é¡µé¢æ ‡é¢˜**: {result['title']}  
**mainContentå…ƒç´ **: {'å·²æ‰¾åˆ°' if result.get('element_found') else 'æœªæ‰¾åˆ°'}  
**çˆ¬å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

{result['content']}
"""
            else:
                # For failed or dummy items, include their details and error, but no content
                status_text = "æˆåŠŸ (æ—§æ‰¹æ¬¡)" if result.get('success', False) else "å¤±è´¥"
                cumulative_content_for_final_report += f"""# {status_text}: {result.get('name', 'Unknown URL')}

**æ¥æº**: {result.get('source', 'Unknown')}  
**URL**: {result.get('url', 'N/A')}  
**é”™è¯¯**: {result.get('error', 'N/A')}  
**çˆ¬å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---
"""
            
            if i < len(all_results):
                cumulative_content_for_final_report += f"\n\n{'='*80}\n\n"

        f.write(cumulative_content_for_final_report)


        if is_completed:
            f.write(f"\n---\n\n## ğŸŠ çˆ¬å–ä»»åŠ¡å·²å…¨éƒ¨å®Œæˆï¼\n\n")
            f.write(f"æ„Ÿè°¢æ‚¨çš„è€å¿ƒç­‰å¾…ï¼Œæ‰€æœ‰ {processed_urls_count} ä¸ªURLå·²å¤„ç†å®Œæ¯•ã€‚\n")
            f.write(f"è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹å„æ‰¹æ¬¡ç›®å½•ä¸­çš„æ–‡ä»¶ã€‚\n")
        else:
            f.write(f"\n---\n\n## â±ï¸ ä»»åŠ¡è¿›è¡Œä¸­...\n\n")
            f.write(f"æ­¤æŠ¥å‘Šå°†éšæ¯ä¸ªæ‰¹æ¬¡çš„å®Œæˆè€Œæ›´æ–°ï¼Œåæ˜ å½“å‰çš„ç´¯è®¡ç»“æœã€‚\n")
            f.write(f"æ‚¨å¯ä»¥æŸ¥çœ‹ `00_OVERALL_SUMMARY.md` è·å–æ›´å®æ—¶çš„è¿›åº¦ã€‚\n")


async def crawl_multiple_webpages_to_markdown(url_list, batch_size=50):
    """
    æ‰¹é‡çˆ¬å–å¤šä¸ªç½‘é¡µçš„mainContentå…ƒç´ å¹¶è¿”å›markdownæ ¼å¼å†…å®¹
    æ¯å¤„ç†æŒ‡å®šæ•°é‡çš„URLåè¿›è¡Œä¸€æ¬¡æ–‡ä»¶ä¿å­˜æ“ä½œï¼Œå¹¶æ”¯æŒä»¥æ‰¹æ¬¡ä¸ºå•ä½çš„æ–­ç‚¹ç»­ä¼ ã€‚
    
    Args:
        url_list: åŒ…å«å­—å…¸çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åº”åŒ…å« 'url', 'source', 'name' ç­‰å­—æ®µ
        batch_size: æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤50ä¸ªURLä¸€æ‰¹
    
    Returns:
        list: åŒ…å«çˆ¬å–ç»“æœçš„åˆ—è¡¨
    """
    if not url_list:
        print("âŒ URLåˆ—è¡¨ä¸ºç©º")
        return []
    
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å– {len(url_list)} ä¸ªç½‘é¡µçš„mainContentå…ƒç´ ...")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size} ä¸ªURL/æ‰¹æ¬¡")
    
    all_results = [] # Accumulates results (can include dummy entries for resumed items)
    total_successful = 0
    total_failed = 0
    
    start_index_for_crawl = 0 # This will be the starting point for the current crawl session
    output_dir = ""
    start_time = ""

    total_urls_to_process = len(url_list) # Total URLs from the input list
    total_batches = (total_urls_to_process + batch_size - 1) // batch_size
    

    # å°è¯•åŠ è½½è¿›åº¦æ—¥å¿—
    progress_data, existing_output_dir = load_progress_log()

    if progress_data and existing_output_dir:
        start_index_for_crawl = progress_data.get("processed_count", 0)
        output_dir = existing_output_dir
        start_time = progress_data.get("start_time", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        total_successful = progress_data.get("successful_count", 0)
        total_failed = progress_data.get("failed_count", 0)

        # Populate all_results with dummy entries for already processed URLs
        # This is crucial for accurate 'processed_count' in summary reports
        for i in range(start_index_for_crawl):
            # Create a placeholder for already processed URLs.
            # We don't need their actual content here, just a 'success' status
            # to keep counts accurate for the summary reports.
            # We assume URLs up to start_index_for_crawl were counted in successful/failed_count
            # from the loaded progress_data.
            # Add basic info to ensure name/url are available for reports, even if dummy
            dummy_item = url_list[i] if i < len(url_list) else {}
            all_results.append({
                'success': True, # Assume success for already processed for general counts
                'batch_num': (i // batch_size) + 1,
                'name': dummy_item.get('name', f"Dummy URL {i+1}"),
                'url': dummy_item.get('final_url', f"dummy_url_{i+1}"),
                'source': dummy_item.get('source', 'Previous Run'),
                'error': None, # No error for dummy success
                'title': 'Previously Processed'
            })


        print(f"âœ… ä»æ–­ç‚¹ç»­ä¼ ï¼Œå°†ä»ç¬¬ {start_index_for_crawl + 1} ä¸ªURLå¼€å§‹å¤„ç†ã€‚")
        print(f"   å·²æˆåŠŸ: {total_successful}, å·²å¤±è´¥: {total_failed}")
    else:
        # åˆ›å»ºæ–°çš„è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = f"crawl_results_{timestamp}"
        start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        os.makedirs(output_dir, exist_ok=True)
        # åˆå§‹åŒ–è¿›åº¦æ—¥å¿—ï¼Œè®°å½•å¼€å§‹æ—¶é—´
        save_progress_log(output_dir, 0, total_urls_to_process, 0, 0, start_time=start_time, output_dir_name=output_dir)
        print(f"ğŸ†• æœªæ‰¾åˆ°æœ‰æ•ˆè¿›åº¦ï¼Œå°†å¼€å§‹æ–°çš„çˆ¬å–å¹¶åˆ›å»ºç›®å½•: {output_dir}")
    
    
    # è°ƒæ•´èµ·å§‹æ‰¹æ¬¡å·
    current_batch_start_index = start_index_for_crawl // batch_size
    
    for batch_num_idx in range(current_batch_start_index, total_batches):
        start_index = batch_num_idx * batch_size # Start index of the current *full* batch in url_list
        end_index = min(start_index + batch_size, total_urls_to_process)
        current_batch_urls = url_list[start_index:end_index] # URLs for the *entire* current batch
        
        print(f"\n{'='*80}")
        print(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_num_idx + 1}/{total_batches}")
        print(f"ğŸ“‹ URLèŒƒå›´: {start_index + 1} - {end_index}")
        print(f"ğŸ“Š å½“å‰æ‰¹æ¬¡å¤§å°: {len(current_batch_urls)}") 
        print(f"{'='*80}")
        
        batch_results_current_run = [] # Collect results only for this batch in the current run
        
        # Process every URL in the current batch (even if partially done before)
        for i, item in enumerate(current_batch_urls):
            global_index = start_index + i + 1 # Correct global index for display and tracking within this batch

            print(f"\n{'.'*60}")
            print(f"å¤„ç†ç¬¬ {global_index}/{total_urls_to_process} ä¸ªURL (æ‰¹æ¬¡å†…ç¬¬ {i+1}/{len(current_batch_urls)} ä¸ª)")
            print(f"æ¥æº: {item.get('source', 'Unknown')}")
            print(f"åç§°: {item.get('name', 'Unknown')}")
            print(f"URL: {item.get('final_url', '')}")
            print(f"{'.'*60}")
            
            if not item.get('final_url'):
                print("âŒ URLä¸ºç©ºï¼Œè·³è¿‡æ­¤é¡¹")
                result = {
                    'success': False,
                    'source': item.get('source', 'Unknown'),
                    'name': item.get('name', 'Unknown'),
                    'url': '',
                    'error': 'URLä¸ºç©º',
                    'batch_num': batch_num_idx + 1 
                }
                batch_results_current_run.append(result)
                total_failed += 1
                continue
            
            # çˆ¬å–å•ä¸ªç½‘é¡µ
            result = await crawl_single_webpage_to_markdown(
                item['final_url'],
                f"{item.get('source', 'Unknown')} - {item.get('name', 'Unknown')}"
            )
            
            # æ·»åŠ é¢å¤–ä¿¡æ¯
            result['name'] = item.get('name', 'Unknown')
            result['batch_num'] = batch_num_idx + 1 
            batch_results_current_run.append(result)
            
            if result['success']:
                total_successful += 1
            else:
                total_failed += 1
            
            # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
            await asyncio.sleep(1)
        
        # After processing all URLs in the current batch:
        # Save current batch's results (always use start_index for batch file naming)
        batch_successful_count = save_batch_results(batch_results_current_run, output_dir, batch_num_idx + 1, start_index)
        
        # Extend all_results with the current batch's results
        all_results.extend(batch_results_current_run) 

        # Update progress log - END OF BATCH update
        processed_count_after_batch = len(all_results)
        is_final_batch = (batch_num_idx == total_batches - 1)
        current_end_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        save_progress_log(output_dir, processed_count_after_batch, total_urls_to_process, total_successful, total_failed, end_time=current_end_time_str if is_final_batch else None, output_dir_name=output_dir)
        
        # Update overall summary (real-time progress report)
        update_overall_summary(output_dir, all_results, total_successful, total_failed, batch_num_idx + 1, total_batches, batch_size, start_time, total_urls_to_process)

        # Update final summary report (cumulative report) after each batch
        generate_final_summary_report(output_dir, all_results, total_successful, total_failed, url_list, batch_size, start_time, current_end_time_str, total_urls_to_process)
        
        print(f"\nğŸ“Š æ‰¹æ¬¡ {batch_num_idx + 1} å®Œæˆç»Ÿè®¡:")
        print(f"âœ… æ‰¹æ¬¡æˆåŠŸ: {batch_successful_count}")
        print(f"âŒ æ‰¹æ¬¡å¤±è´¥: {len(batch_results_current_run) - batch_successful_count}")
        print(f"ğŸ“ˆ æ€»ä½“è¿›åº¦: {processed_count_after_batch}/{total_urls_to_process} ({(processed_count_after_batch/total_urls_to_process*100):.1f}%)")
        print(f"ğŸ“Š ç´¯è®¡æˆåŠŸ: {total_successful}")
        print(f"ğŸ“Š ç´¯è®¡å¤±è´¥: {total_failed}")
        
        # Pause between batches
        if batch_num_idx < total_batches - 1:
            print("â¸ï¸  æ‰¹æ¬¡é—´æš‚åœ 3 ç§’...")
            await asyncio.sleep(3)
    
    # Final updates after all batches are done
    final_end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    save_progress_log(output_dir, total_urls_to_process, total_urls_to_process, total_successful, total_failed, end_time=final_end_time, output_dir_name=output_dir)
    update_overall_summary(output_dir, all_results, total_successful, total_failed, total_batches, total_batches, batch_size, start_time, total_urls_to_process)
    generate_final_summary_report(output_dir, all_results, total_successful, total_failed, url_list, batch_size, start_time, final_end_time, total_urls_to_process)

    print(f"\nğŸ‰ æ‰¹é‡çˆ¬å–å…¨éƒ¨å®Œæˆï¼")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   æ€»æ•°é‡: {total_urls_to_process}")
    print(f"   âœ… æˆåŠŸ: {total_successful}")
    print(f"   âŒ å¤±è´¥: {total_failed}")
    print(f"   ğŸ“ˆ æˆåŠŸç‡: {(total_successful/total_urls_to_process*100):.2f}%")
    print(f"   ğŸ—‚ï¸  æ‰¹æ¬¡æ•°: {total_batches}")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨ç›®å½•: {output_dir}")
    print(f"ğŸ“„ æ€»ä½“æ±‡æ€»æ–‡ä»¶: {os.path.join(output_dir, '00_OVERALL_SUMMARY.md')}")
    print(f"ğŸ“„ æœ€ç»ˆè¯¦ç»†æŠ¥å‘Š: {os.path.join(output_dir, '00_FINAL_SUMMARY.md')}")
    
    return all_results

# åŒæ­¥ç‰ˆæœ¬å‡½æ•°ï¼ˆå¦‚æœéœ€è¦åœ¨åŒæ­¥ç¯å¢ƒä¸­ä½¿ç”¨ï¼‰
def crawl_multiple_webpages_sync(url_list, batch_size=50):
    """
    åŒæ­¥ç‰ˆæœ¬çš„æ‰¹é‡çˆ¬å–å‡½æ•°
    """
    return asyncio.run(crawl_multiple_webpages_to_markdown(url_list, batch_size))

# ä¸»å‡½æ•°
async def main():
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ‰¹é‡çˆ¬å–ä»»åŠ¡
    """
    # ä»JSONæ–‡ä»¶è·å–URLs
    json_file_path = 'jyxx_final_urls.json'
    jyxx_urls = get_urls(json_file_path)

    example_urls = [
            {
                "source": "ç¬¬ 1 é¡µç¬¬ 1 ä¸ªå…ƒç´ ",
                "name": "é‡åº†å¸‚ä¸‰å³¡åº“åŒºæ°´æºå·¥ç¨‹æ™ºç®¡èƒ½åŠ›å»ºè®¾å…ˆè¡Œå…ˆè¯•é¡¹ç›®ï¼ˆæ¶ªé™µåŒºï¼‰æ‹›æ ‡è®¡åˆ’è¡¨",
                "final_url": "https://www.cqggzy.com/jyxx/004002/004002008/20250531/9d76df21-eb96-4384-aa1d-3e1e852cd414.html"
            },
            {
                "source": "ç¬¬ 1 é¡µç¬¬ 2 ä¸ªå…ƒç´ ",
                "name": "å¨æˆ¿åŠç”Ÿæ´»è®¾æ–½è®¾å¤‡(WLQ25A00353)è¯¢ä»·å…¬å‘Š",
                "final_url": "https://www.cqggzy.com/jyxx/004005/004005001/20250531/1504897345949839360.html"
            }
        ]
    
    # æ‰§è¡Œæ‰¹é‡çˆ¬å–ï¼Œå¯ä»¥è‡ªå®šä¹‰æ‰¹æ¬¡å¤§å°
    # results = await crawl_multiple_webpages_to_markdown(example_urls, batch_size=50)
    results = await crawl_multiple_webpages_to_markdown(jyxx_urls, batch_size=50)
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å¼‚æ­¥è¿è¡Œ
    results = asyncio.run(main())
    
    # æˆ–è€…ç›´æ¥è°ƒç”¨æ‰¹é‡çˆ¬å–å‡½æ•°
    # your_url_list = [...]  # ä½ çš„URLåˆ—è¡¨
    # results = crawl_multiple_webpages_sync(your_url_list, batch_size=50)
