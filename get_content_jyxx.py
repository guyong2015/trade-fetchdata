import json
# åŠ å…¥äº†beautifulsoup4å’Œhtml2textåº“ï¼Œç”¨äºå¤„ç†HTMLå†…å®¹å’Œè½¬æ¢ä¸ºmarkdownæ ¼å¼ã€‚
import asyncio
from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
import html2text
import os
from datetime import datetime


def get_urls(json_file_path):
    """
    ä»æŒ‡å®šçš„JSONæ–‡ä»¶ä¸­è·å–urlsé”®å€¼
    
    Args:
        json_file_path: JSONæ–‡ä»¶çš„è·¯å¾„
        
    Returns:
        list: åŒ…å«URLä¿¡æ¯çš„åˆ—è¡¨
    """
    try:
        # è¯»å–JSONæ–‡ä»¶
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # è·å–urlsé”®å€¼
        urls = data.get('urls', [])
        
        print(f"âœ… æˆåŠŸä» {json_file_path} è·å– {len(urls)} ä¸ªURL")
        return urls
    
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {json_file_path} - {str(e)}")
        return []
async def crawl_single_webpage_to_markdown(url, source_info=""):
    """
    ä½¿ç”¨crawl4aiçˆ¬å–æŒ‡å®šç½‘é¡µçš„mainContentå…ƒç´ å¹¶è¿”å›markdownæ ¼å¼å†…å®¹
    """
    try:
        # åˆ›å»ºå¼‚æ­¥çˆ¬è™«å®ä¾‹
        async with AsyncWebCrawler(verbose=True) as crawler:
            # çˆ¬å–ç½‘é¡µ
            result = await crawler.arun(
                url=url,
                # ç­‰å¾…mainContentå…ƒç´ åŠ è½½å®Œæˆ
                wait_for_selector="#mainContent",
                timeout=30000,  # 30ç§’è¶…æ—¶
                # è®¾ç½®ç”¨æˆ·ä»£ç†
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                # ä½¿ç”¨CSSé€‰æ‹©å™¨åªæå–mainContentå†…å®¹
                css_selector="#mainContent"
            )
            
            if result.success:
                print(f"âœ… çˆ¬å–æˆåŠŸ! {source_info}")
                print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
                print(f"ğŸ”— URL: {url}")
                
                # ä»HTMLå†…å®¹ä¸­æå–mainContentå¹¶è½¬æ¢ä¸ºmarkdown
                soup = BeautifulSoup(result.html, 'html.parser')
                maincontent_element = soup.find(id='mainContent')
                
                if maincontent_element:
                    print("ğŸ” æ‰¾åˆ° mainContent å…ƒç´ ")
                    
                    # è®¾ç½®html2textè½¬æ¢å™¨
                    h = html2text.HTML2Text()
                    h.ignore_links = False
                    h.ignore_images = False
                    h.body_width = 0  # ä¸é™åˆ¶è¡Œå®½
                    
                    # è½¬æ¢mainContentä¸ºmarkdown
                    print("ğŸ“‹ å¤„ç† mainContent å…ƒç´ ...")
                    maincontent_html = str(maincontent_element)
                    markdown_content = h.handle(maincontent_html)
                    
                    print(f"ğŸ“ è½¬æ¢åå†…å®¹é•¿åº¦: {len(markdown_content)} å­—ç¬¦")
                    
                    return {
                        'success': True,
                        'source': source_info,
                        'url': url,
                        'title': result.metadata.get('title', 'N/A'),
                        'content': markdown_content,
                        'element_found': True
                    }
                else:
                    print(f"âŒ æœªæ‰¾åˆ°mainContentå…ƒç´ : {source_info}")
                    return {
                        'success': False,
                        'source': source_info,
                        'url': url,
                        'error': 'æœªæ‰¾åˆ°mainContentå…ƒç´ '
                    }
            else:
                print(f"âŒ çˆ¬å–å¤±è´¥: {source_info} - {result.error_message}")
                return {
                    'success': False,
                    'source': source_info,
                    'url': url,
                    'error': result.error_message
                }
                
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {source_info} - {str(e)}")
        return {
            'success': False,
            'source': source_info,
            'url': url,
            'error': str(e)
        }

async def crawl_multiple_webpages_to_markdown(url_list):
    """
    æ‰¹é‡çˆ¬å–å¤šä¸ªç½‘é¡µçš„mainContentå…ƒç´ å¹¶è¿”å›markdownæ ¼å¼å†…å®¹
    
    Args:
        url_list: åŒ…å«å­—å…¸çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åº”åŒ…å« 'url', 'source', 'name' ç­‰å­—æ®µ
    
    Returns:
        list: åŒ…å«çˆ¬å–ç»“æœçš„åˆ—è¡¨
    """
    if not url_list:
        print("âŒ URLåˆ—è¡¨ä¸ºç©º")
        return []
    
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å– {len(url_list)} ä¸ªç½‘é¡µçš„mainContentå…ƒç´ ...")
    
    results = []
    all_markdown_content = ""
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"crawl_results_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    
    for i, item in enumerate(url_list, 1):
        print(f"\n{'='*60}")
        print(f"å¤„ç†ç¬¬ {i}/{len(url_list)} ä¸ªURL")
        print(f"æ¥æº: {item.get('source', 'Unknown')}")
        print(f"åç§°: {item.get('name', 'Unknown')}")
        print(f"URL: {item.get('final_url', '')}")
        print(f"{'='*60}")
        
        if not item.get('final_url'):
            print("âŒ URLä¸ºç©ºï¼Œè·³è¿‡æ­¤é¡¹")
            results.append({
                'success': False,
                'source': item.get('source', 'Unknown'),
                'name': item.get('name', 'Unknown'),
                'url': '',
                'error': 'URLä¸ºç©º'
            })
            continue
        
        # çˆ¬å–å•ä¸ªç½‘é¡µ
        result = await crawl_single_webpage_to_markdown(
            item['final_url'],
            f"{item.get('source', 'Unknown')} - {item.get('name', 'Unknown')}"
        )
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯
        result['name'] = item.get('name', 'Unknown')
        results.append(result)
        
        # å¦‚æœçˆ¬å–æˆåŠŸï¼Œä¿å­˜å•ä¸ªæ–‡ä»¶å¹¶æ·»åŠ åˆ°æ€»å†…å®¹ä¸­
        if result['success']:
            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            safe_filename = f"{i:03d}_{item.get('source', 'unknown').replace('/', '_').replace(' ', '_')}"
            file_path = os.path.join(output_dir, f"{safe_filename}.md")
            
            # æ„å»ºæ–‡ä»¶å†…å®¹
            file_content = f"""# {item.get('name', 'Unknown')}

**æ¥æº**: {item.get('source', 'Unknown')}  
**URL**: {item['final_url']}  
**é¡µé¢æ ‡é¢˜**: {result['title']}  
**mainContentå…ƒç´ **: {'å·²æ‰¾åˆ°' if result.get('element_found') else 'æœªæ‰¾åˆ°'}  
**çˆ¬å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

{result['content']}
"""
            
            # ä¿å­˜å•ä¸ªæ–‡ä»¶
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(file_content)
            print(f"ğŸ’¾ å†…å®¹å·²ä¿å­˜åˆ° {file_path}")
            
            # æ·»åŠ åˆ°æ€»å†…å®¹ä¸­
            all_markdown_content += f"\n\n{'='*80}\n"
            all_markdown_content += file_content
        
        # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
        await asyncio.sleep(1)
    
    # ä¿å­˜æ±‡æ€»æ–‡ä»¶
    summary_file = os.path.join(output_dir, "00_summary.md")
    with open(summary_file, "w", encoding="utf-8") as f:
        f.write(f"""# æ‰¹é‡çˆ¬å–mainContentæ±‡æ€»æŠ¥å‘Š

**çˆ¬å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**æ€»URLæ•°é‡**: {len(url_list)}  
**æˆåŠŸæ•°é‡**: {sum(1 for r in results if r['success'])}  
**å¤±è´¥æ•°é‡**: {sum(1 for r in results if not r['success'])}  

## è¯¦ç»†ç»“æœ

""")
        
        for i, result in enumerate(results, 1):
            status = "âœ… æˆåŠŸ" if result['success'] else "âŒ å¤±è´¥"
            error_msg = f" - {result.get('error', '')}" if not result['success'] else ""
            f.write(f"{i}. {status} | {result.get('source', 'Unknown')} | {result.get('name', 'Unknown')}{error_msg}\n")
        
        f.write(f"\n---\n\n{all_markdown_content}")
    
    print(f"\nğŸ“Š æ‰¹é‡çˆ¬å–å®Œæˆï¼")
    print(f"âœ… æˆåŠŸ: {sum(1 for r in results if r['success'])}")
    print(f"âŒ å¤±è´¥: {sum(1 for r in results if not r['success'])}")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨ç›®å½•: {output_dir}")
    print(f"ğŸ“„ æ±‡æ€»æ–‡ä»¶: {summary_file}")
    
    return results

# åŒæ­¥ç‰ˆæœ¬å‡½æ•°ï¼ˆå¦‚æœéœ€è¦åœ¨åŒæ­¥ç¯å¢ƒä¸­ä½¿ç”¨ï¼‰
def crawl_multiple_webpages_sync(url_list):
    """
    åŒæ­¥ç‰ˆæœ¬çš„æ‰¹é‡çˆ¬å–å‡½æ•°
    """
    return asyncio.run(crawl_multiple_webpages_to_markdown(url_list))

# ä¸»å‡½æ•°
async def main():
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ‰¹é‡çˆ¬å–ä»»åŠ¡
    """
    # ä»JSONæ–‡ä»¶è·å–URLs
    json_file_path = 'jyxx_final_urls.json'
    jyxx_urls = get_urls(json_file_path)

    example_urls = [
            {
                "source": "ç¬¬ 1 é¡µç¬¬ 1 ä¸ªå…ƒç´ ",
                "name": "é‡åº†å¸‚ä¸‰å³¡åº“åŒºæ°´æºå·¥ç¨‹æ™ºç®¡èƒ½åŠ›å»ºè®¾å…ˆè¡Œå…ˆè¯•é¡¹ç›®ï¼ˆæ¶ªé™µåŒºï¼‰æ‹›æ ‡è®¡åˆ’è¡¨",
                "final_url": "https://www.cqggzy.com/jyxx/004002/004002008/20250531/9d76df21-eb96-4384-aa1d-3e1e852cd414.html"
            },
            {
                "source": "ç¬¬ 1 é¡µç¬¬ 2 ä¸ªå…ƒç´ ",
                "name": "å¨æˆ¿åŠç”Ÿæ´»è®¾æ–½è®¾å¤‡(WLQ25A00353)è¯¢ä»·å…¬å‘Š",
                "final_url": "https://www.cqggzy.com/jyxx/004005/004005001/20250531/1504897345949839360.html"
            }
        ]
    
    # æ‰§è¡Œæ‰¹é‡çˆ¬å–
    # results = await crawl_multiple_webpages_to_markdown(example_urls)
    results = await crawl_multiple_webpages_to_markdown(jyxx_urls)
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å¼‚æ­¥è¿è¡Œ
    results = asyncio.run(main())
    
    # æˆ–è€…ç›´æ¥è°ƒç”¨æ‰¹é‡çˆ¬å–å‡½æ•°
    # your_url_list = [...]  # ä½ çš„URLåˆ—è¡¨
    # results = crawl_multiple_webpages_sync(your_url_list)