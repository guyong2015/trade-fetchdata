# åŠ å…¥äº†beautifulsoup4å’Œhtml2textåº“ï¼Œç”¨äºå¤„ç†HTMLå†…å®¹å’Œè½¬æ¢ä¸ºmarkdownæ ¼å¼ã€‚
import asyncio
from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
import html2text

async def crawl_webpage_to_markdown():
    """
    ä½¿ç”¨crawl4aiçˆ¬å–æŒ‡å®šç½‘é¡µçš„mainContentå…ƒç´ å¹¶è¿”å›markdownæ ¼å¼å†…å®¹
    """
    url = "https://www.cqggzy.com/jyxx/004002/004002008/20250531/9d76df21-eb96-4384-aa1d-3e1e852cd414.html"
    url = "https://www.cqggzy.com/jyxx/004005/004005001/20250531/1504897345949839360.html"

    # url = "https://www.cqggzy.com/jumpnew.html?infoid=9d76df21-eb96-4384-aa1d-3e1e852cd414&categorynum=004002008"
    try:
        # åˆ›å»ºå¼‚æ­¥çˆ¬è™«å®ä¾‹
        async with AsyncWebCrawler(verbose=True) as crawler:
            # çˆ¬å–ç½‘é¡µ
            result = await crawler.arun(
                url=url,
                # ç­‰å¾…mainContentå…ƒç´ åŠ è½½å®Œæˆ
                wait_for_selector="#mainContent",
                timeout=30000,  # 30ç§’è¶…æ—¶
                # è®¾ç½®ç”¨æˆ·ä»£ç†
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                # ä½¿ç”¨CSSé€‰æ‹©å™¨åªæå–mainContentå†…å®¹
                css_selector="#mainContent"
            )
            
            if result.success:
                print("âœ… çˆ¬å–æˆåŠŸ!")
                print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
                print(f"ğŸ”— URL: {url}")
                
                # ä»HTMLå†…å®¹ä¸­æå–mainContentå¹¶è½¬æ¢ä¸ºmarkdown
                from bs4 import BeautifulSoup
                import html2text
                
                # è§£æHTMLå†…å®¹
                soup = BeautifulSoup(result.html, 'html.parser')
                maincontent_element = soup.find(id='mainContent')
                
                if maincontent_element:
                    print("ğŸ” æ‰¾åˆ° mainContent å…ƒç´ ")
                    
                    # è®¾ç½®html2textè½¬æ¢å™¨
                    h = html2text.HTML2Text()
                    h.ignore_links = False
                    h.ignore_images = False
                    h.body_width = 0  # ä¸é™åˆ¶è¡Œå®½
                    
                    # è½¬æ¢mainContentä¸ºmarkdown
                    print("ğŸ“‹ å¤„ç† mainContent å…ƒç´ ...")
                    maincontent_html = str(maincontent_element)
                    markdown_content = h.handle(maincontent_html)
                    
                    print(f"ğŸ“ è½¬æ¢åå†…å®¹é•¿åº¦: {len(markdown_content)} å­—ç¬¦")
                    print("\n" + "="*50 + " MAINCONTENT MARKDOWN å†…å®¹ " + "="*50)
                    print(markdown_content)
                    print("="*111)
                    
                    # ä¿å­˜åˆ°æ–‡ä»¶
                    with open("maincontent.md", "w", encoding="utf-8") as f:
                        f.write(markdown_content)
                    print("\nğŸ’¾ mainContentå†…å®¹å·²ä¿å­˜åˆ° maincontent.md æ–‡ä»¶")
                    
                    return markdown_content
                else:
                    print("âŒ æœªæ‰¾åˆ°id='mainContent'çš„å…ƒç´ ")
                    return None
            else:
                print(f"âŒ çˆ¬å–å¤±è´¥: {result.error_message}")
                return None
                
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        return None

# åŒæ­¥ç‰ˆæœ¬å‡½æ•°ï¼ˆå¦‚æœéœ€è¦åœ¨åŒæ­¥ç¯å¢ƒä¸­ä½¿ç”¨ï¼‰
def crawl_webpage_sync():
    """
    åŒæ­¥ç‰ˆæœ¬çš„çˆ¬å–å‡½æ•°
    """
    return asyncio.run(crawl_webpage_to_markdown())

# ä¸»å‡½æ•°
async def main():
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œçˆ¬å–ä»»åŠ¡
    """
    print("ğŸš€ å¼€å§‹çˆ¬å–ç½‘é¡µçš„mainContentå…ƒç´ ...")
    markdown_content = await crawl_webpage_to_markdown()
    
    if markdown_content:
        print("\nâœ… mainContentå…ƒç´ çˆ¬å–å®Œæˆ!")
    else:
        print("\nâŒ mainContentå…ƒç´ çˆ¬å–å¤±è´¥!")

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    # å¼‚æ­¥è¿è¡Œ
    asyncio.run(main())
    
    # æˆ–è€…ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬
    # crawl_webpage_sync()