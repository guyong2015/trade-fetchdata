# é€šè¿‡æ¨¡æ‹Ÿç‚¹å‡»è·å–æ‰€æœ‰æ–°æ ‡ç­¾é¡µçš„æœ€ç»ˆé‡å®šå‘URLï¼Œæ”¯æŒç¿»é¡µæ“ä½œï¼Œå¹¶ä¿å­˜åˆ°JSONæ–‡ä»¶
# ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ”¯æŒåˆ†æ‰¹ä¿å­˜ã€æ–­ç‚¹ç»­ä¼ ã€å®¹é”™å¤„ç†
from playwright.sync_api import sync_playwright
import json
import os
import time
from datetime import datetime

def get_final_redirect_url(page, initial_url, wait_time=5):
    """
    è·å–å•ä¸ªURLçš„æœ€ç»ˆé‡å®šå‘URL
    
    Args:
        page: Playwrighté¡µé¢å¯¹è±¡
        initial_url: åˆå§‹URL
        wait_time: ç­‰å¾…é‡å®šå‘çš„æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        åŒ…å«é‡å®šå‘ä¿¡æ¯çš„å­—å…¸
    """
    redirect_chain = [initial_url]
    current_url = initial_url
    
    try:
        # è®¿é—®åˆå§‹URL
        page.goto(initial_url, wait_until='domcontentloaded')
        current_url = page.url
        
        if current_url != initial_url:
            redirect_chain.append(current_url)
        
        # ç­‰å¾…å¯èƒ½çš„JavaScripté‡å®šå‘ï¼Œåˆ†æ®µæ£€æŸ¥
        for i in range(wait_time * 2):  # æ¯0.5ç§’æ£€æŸ¥ä¸€æ¬¡
            time.sleep(0.5)
            new_url = page.url
            
            if new_url != current_url:
                print(f"    æ£€æµ‹åˆ°é‡å®šå‘: {current_url} -> {new_url}")
                redirect_chain.append(new_url)
                current_url = new_url
        
        # æœ€ç»ˆç­‰å¾…ç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
        try:
            page.wait_for_load_state('networkidle', timeout=3000)
        except:
            pass  # å¿½ç•¥è¶…æ—¶
        
        final_url = page.url
        if final_url != current_url:
            redirect_chain.append(final_url)
        
        # è·å–é¡µé¢æ ‡é¢˜
        try:
            page_title = page.title()
        except:
            page_title = "æ— æ³•è·å–æ ‡é¢˜"
        
        return {
            'success': True,
            'initial_url': initial_url,
            'final_url': final_url,
            'redirect_chain': redirect_chain,
            'total_redirects': len(redirect_chain) - 1,
            'page_title': page_title
        }
    
    except Exception as e:
        return {
            'success': False,
            'initial_url': initial_url,
            'final_url': initial_url,
            'error': str(e),
            'redirect_chain': redirect_chain,
            'total_redirects': 0,
            'page_title': 'è·å–å¤±è´¥'
        }

def load_existing_data(filename):
    """
    åŠ è½½å·²å­˜åœ¨çš„æ•°æ®æ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
    
    Args:
        filename: JSONæ–‡ä»¶å
        
    Returns:
        tuple: (ç°æœ‰æ•°æ®åˆ—è¡¨, æœ€åå¤„ç†çš„é¡µç )
    """
    if os.path.exists(filename):
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            existing_urls = data.get('urls', [])
            
            # è®¡ç®—æœ€åå¤„ç†çš„é¡µç 
            last_page = 0
            if existing_urls:
                # ä»æœ€åä¸€æ¡è®°å½•ä¸­æå–é¡µç 
                last_source = existing_urls[-1].get('source', '')
                if 'ç¬¬ ' in last_source and ' é¡µ' in last_source:
                    try:
                        page_part = last_source.split('ç¬¬ ')[1].split(' é¡µ')[0]
                        last_page = int(page_part)
                    except:
                        pass
            
            print(f"ğŸ“‚ å‘ç°å·²å­˜åœ¨çš„æ•°æ®æ–‡ä»¶: {filename}")
            print(f"ğŸ“Š å·²æœ‰æ•°æ®: {len(existing_urls)} æ¡")
            print(f"ğŸ“„ æœ€åå¤„ç†é¡µç : {last_page}")
            
            return existing_urls, last_page
            
        except Exception as e:
            print(f"âš ï¸ è¯»å–å·²å­˜åœ¨æ•°æ®æ—¶å‡ºé”™: {e}")
            return [], 0
    
    return [], 0

def save_urls_to_json_batch(urls, filename="jyxx_final_urls.json", timebegin=None, is_final=False):
    """
    æ‰¹é‡ä¿å­˜URLæ•°æ®åˆ°JSONæ–‡ä»¶
    
    Args:
        urls: URLåˆ—è¡¨
        filename: è¾“å‡ºæ–‡ä»¶å
        timebegin: å¼€å§‹æ—¶é—´
        is_final: æ˜¯å¦ä¸ºæœ€ç»ˆä¿å­˜
    """
    try:
        # ç»Ÿè®¡æ•°æ®
        total_count = len(urls)
        success_count = len([url for url in urls if url.get('final_url') != 'è·å–å¤±è´¥'])
        failed_count = total_count - success_count
        
        # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
        data = {
            "metadata": {
                "timebegin": timebegin or datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "total_count": total_count,
                "success_count": success_count,
                "failed_count": failed_count,
                "description": "è·å–é‡å®šå‘åçš„æœ€ç»ˆURLæ•°æ®",
                "is_complete": is_final
            },
            "statistics": {
                "success_rate": f"{(success_count/total_count*100):.1f}%" if total_count > 0 else "0%",
            },
            "urls": urls
        }
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        if is_final:
            print(f"\n=== ğŸ“„ æœ€ç»ˆæ•°æ®å·²ä¿å­˜åˆ° {filename} ===")
        else:
            print(f"\nğŸ’¾ æ‰¹é‡ä¿å­˜: {total_count} æ¡æ•°æ®å·²ä¿å­˜åˆ° {filename}")
        
        print(f"ğŸ“… ä¿å­˜æ—¶é—´: {data['metadata']['timestamp']}")
        print(f"ğŸ“Š æ€»æ•°é‡: {data['metadata']['total_count']}")
        print(f"âœ… æˆåŠŸæ•°é‡: {data['metadata']['success_count']}")
        print(f"âŒ å¤±è´¥æ•°é‡: {data['metadata']['failed_count']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {data['statistics']['success_rate']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜JSONæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

def get_all_popup_urls_with_redirect(target_url, max_pages, link_selector=".class-item", 
                                   batch_size=10, output_filename="jyxx_final_urls.json", 
                                   resume=True):
    """
    è·å–æ‰€æœ‰åŒ¹é…å…ƒç´ ç‚¹å‡»åæ‰“å¼€çš„æ–°æ ‡ç­¾é¡µURLï¼Œå¹¶è·Ÿè¸ªé‡å®šå‘åˆ°æœ€ç»ˆURL
    æ”¯æŒåˆ†æ‰¹ä¿å­˜å’Œæ–­ç‚¹ç»­ä¼ 
    
    Args:
        target_url: ç›®æ ‡ç½‘é¡µURL
        link_selector: é“¾æ¥é€‰æ‹©å™¨ï¼Œé»˜è®¤ä¸º.class-item
        max_pages: æœ€å¤§å¤„ç†é¡µæ•°
        batch_size: æ‰¹é‡ä¿å­˜çš„é¡µæ•°ï¼ˆæ¯å¤„ç†å¤šå°‘é¡µä¿å­˜ä¸€æ¬¡ï¼‰
        output_filename: è¾“å‡ºæ–‡ä»¶å
        resume: æ˜¯å¦æ”¯æŒæ–­ç‚¹ç»­ä¼ 
    
    Returns:
        åŒ…å«æ‰€æœ‰URLä¿¡æ¯çš„åˆ—è¡¨
    """
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ–­ç‚¹ç»­ä¼ 
    all_urls = []
    start_page = 0
    
    if resume:
        existing_urls, last_page = load_existing_data(output_filename)
        if existing_urls:
            response = input(f"å‘ç°å·²å­˜åœ¨æ•°æ®ï¼ˆ{len(existing_urls)}æ¡ï¼Œæœ€åå¤„ç†ç¬¬{last_page}é¡µï¼‰ï¼Œæ˜¯å¦ç»§ç»­ä»ç¬¬{last_page+1}é¡µå¼€å§‹ï¼Ÿ(y/n): ").lower()
            if response == 'y':
                all_urls = existing_urls
                start_page = last_page
                print(f"ğŸ”„ ä»ç¬¬ {start_page + 1} é¡µå¼€å§‹ç»§ç»­å¤„ç†...")
            else:
                print("ğŸ†• é‡æ–°å¼€å§‹å¤„ç†...")
    
    with sync_playwright() as p:
        # è®¾ç½®æ›´çœŸå®çš„æµè§ˆå™¨æŒ‡çº¹
        browser = p.chromium.launch(
            headless=False,  # å¯ä»¥æ”¹ä¸ºTrueè¿›è¡Œæ— å¤´æ¨¡å¼
            args=[
                "--disable-blink-features=AutomationControlled",
                "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            ]
        )
        context = browser.new_context(
            viewport={"width": 1920, "height": 1080},
            locale="zh-CN"
        )
        
        # åˆ›å»ºä¸»é¡µé¢
        main_page = context.new_page()
        # åˆ›å»ºç”¨äºé‡å®šå‘è·Ÿè¸ªçš„é¡µé¢
        redirect_page = context.new_page()
        
        try:
            # è®¿é—®ç›®æ ‡ç½‘é¡µ
            main_page.goto(target_url, wait_until="domcontentloaded")
            main_page.wait_for_load_state("networkidle", timeout=8000)
            main_page.wait_for_selector(".info-item", state="attached", timeout=8000)
            main_page.wait_for_timeout(1000)

            # å¦‚æœéœ€è¦è·³åˆ°æŒ‡å®šé¡µé¢
            if start_page > 0:
                print(f"ğŸ” æ­£åœ¨è·³è½¬åˆ°ç¬¬ {start_page + 1} é¡µ...")
                for skip_page in range(start_page):
                    try:
                        next_button = main_page.query_selector('a.next')
                        if next_button and 'disabled' not in (next_button.get_attribute('class') or ''):
                            next_button.click()
                            main_page.wait_for_load_state("networkidle", timeout=8000)
                            main_page.wait_for_timeout(1000)
                        else:
                            print(f"âš ï¸ æ— æ³•è·³è½¬åˆ°ç¬¬ {start_page + 1} é¡µï¼Œä»å½“å‰é¡µå¼€å§‹")
                            break
                    except Exception as e:
                        print(f"âš ï¸ è·³è½¬é¡µé¢æ—¶å‡ºé”™: {e}")
                        break
            
            # å¾ªç¯å¤„ç†æ¯ä¸€é¡µ
            for page_num in range(start_page, max_pages):
                print(f"\n=== æ­£åœ¨å¤„ç†ç¬¬ {page_num + 1} é¡µ ===")
                
                try:
                    # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                    main_page.wait_for_selector(".info-item", state="attached", timeout=8000)
                    main_page.wait_for_timeout(1000)
                    
                    # è·å–å½“å‰é¡µé¢æ‰€æœ‰åŒ¹é…çš„å…ƒç´ 
                    elements = main_page.query_selector_all('[class="info-item"]')
                    print(f"ç¬¬ {page_num + 1} é¡µæ‰¾åˆ° {len(elements)} ä¸ªåŒ¹é…çš„å…ƒç´ ")
                    
                    # å¤„ç†å½“å‰é¡µé¢çš„æ¯ä¸ªå…ƒç´ 
                    for i, element in enumerate(elements):
                        element_info = {
                            'source': f'ç¬¬ {page_num + 1} é¡µç¬¬ {i+1} ä¸ªå…ƒç´ ',
                            'name': 'æœªçŸ¥',
                            'final_url': 'è·å–å¤±è´¥',
                        }
                        
                        try:
                            print(f"  æ­£åœ¨å¤„ç†ç¬¬ {page_num + 1} é¡µçš„ç¬¬ {i+1} ä¸ªå…ƒç´ ...")
                            
                            # è·å–æ ‡é¢˜
                            title_element = element.query_selector('a')
                            if title_element:
                                title = title_element.get_attribute('title') or title_element.inner_text().strip()
                                element_info['name'] = title
                                print(f"    æ ‡é¢˜: {title}")
                            
                            # ç›‘å¬æ–°é¡µé¢æ‰“å¼€äº‹ä»¶
                            with main_page.expect_popup() as new_page_info:
                                element.click()
                            
                            # è·å–æ–°é¡µé¢å’Œåˆå§‹URL
                            new_page = new_page_info.value
                            initial_url = new_page.url
                            
                            print(f"    åˆå§‹URL: {initial_url}")
                            
                            # å…³é—­å¼¹å‡ºçš„é¡µé¢ï¼Œæˆ‘ä»¬ç”¨ä¸“é—¨çš„é¡µé¢æ¥è·Ÿè¸ªé‡å®šå‘
                            new_page.close()
                            
                            # ä½¿ç”¨ä¸“é—¨çš„é¡µé¢è·Ÿè¸ªé‡å®šå‘
                            print(f"    å¼€å§‹è·Ÿè¸ªé‡å®šå‘...")
                            redirect_info = get_final_redirect_url(redirect_page, initial_url, wait_time=3)
                            
                            element_info['final_url'] = redirect_info['final_url']
                            
                            if redirect_info['success']:
                                print(f"    âœ… æœ€ç»ˆURL: {redirect_info['final_url']}")
                                if redirect_info['total_redirects'] > 0:
                                    print(f"    ğŸ”„ ç»è¿‡ {redirect_info['total_redirects']} æ¬¡é‡å®šå‘")
                                print(f"    ğŸ“„ é¡µé¢æ ‡é¢˜: {redirect_info['page_title']}")
                            else:
                                print(f"    âŒ é‡å®šå‘è·Ÿè¸ªå¤±è´¥: {redirect_info.get('error', 'æœªçŸ¥é”™è¯¯')}")
                            
                            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œé¿å…æ“ä½œè¿‡å¿«
                            main_page.wait_for_timeout(1000)
                            
                        except Exception as e:
                            print(f"    âŒ å¤„ç†ç¬¬ {page_num + 1} é¡µç¬¬ {i+1} ä¸ªå…ƒç´ æ—¶å‡ºé”™: {e}")
                        
                        all_urls.append(element_info)
                    
                    print(f"ç¬¬ {page_num + 1} é¡µå¤„ç†å®Œæˆ")
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰¹é‡ä¿å­˜
                    if (page_num + 1) % batch_size == 0:
                        print(f"\nğŸ’¾ è¾¾åˆ°æ‰¹é‡ä¿å­˜æ¡ä»¶ï¼ˆæ¯{batch_size}é¡µä¿å­˜ä¸€æ¬¡ï¼‰")
                        save_success = save_urls_to_json_batch(all_urls, output_filename, 
                                                             datetime.now().strftime("%Y-%m-%d %H:%M:%S"), 
                                                             is_final=False)
                        if save_success:
                            print(f"âœ… æ‰¹é‡ä¿å­˜æˆåŠŸï¼å·²å¤„ç† {page_num + 1} é¡µ")
                        else:
                            print(f"âŒ æ‰¹é‡ä¿å­˜å¤±è´¥ï¼")
                    
                    # å¦‚æœä¸æ˜¯æœ€åä¸€é¡µï¼Œç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®
                    if page_num < max_pages - 1:
                        try:
                            print(f"  æ­£åœ¨ç‚¹å‡»ç¬¬ {page_num + 1} æ¬¡ä¸‹ä¸€é¡µæŒ‰é’®...")
                            
                            # æ£€æŸ¥ä¸‹ä¸€é¡µæŒ‰é’®æ˜¯å¦å­˜åœ¨ä¸”å¯ç‚¹å‡»
                            next_button = main_page.query_selector('a.next')
                            if next_button:
                                # æ£€æŸ¥æŒ‰é’®æ˜¯å¦è¢«ç¦ç”¨
                                is_disabled = next_button.get_attribute('class')
                                if 'disabled' not in (is_disabled or ''):
                                    # æ»šåŠ¨åˆ°æŒ‰é’®ä½ç½®ï¼Œç¡®ä¿å¯è§
                                    next_button.scroll_into_view_if_needed()
                                    main_page.wait_for_timeout(500)
                                    
                                    # ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®
                                    next_button.click()
                                    
                                    # ç­‰å¾…é¡µé¢åŠ è½½
                                    main_page.wait_for_load_state("networkidle", timeout=8000)
                                    main_page.wait_for_timeout(2000)
                                    
                                    print(f"  âœ… æˆåŠŸç‚¹å‡»ä¸‹ä¸€é¡µï¼Œå³å°†å¤„ç†ç¬¬ {page_num + 2} é¡µ")
                                else:
                                    print("  âš ï¸ ä¸‹ä¸€é¡µæŒ‰é’®å·²ç¦ç”¨ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                                    break
                            else:
                                print("  âš ï¸ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                                break
                                
                        except Exception as e:
                            print(f"  âŒ ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®æ—¶å‡ºé”™: {e}")
                            # å‡ºé”™æ—¶ä¹Ÿä¿å­˜å½“å‰æ•°æ®
                            print("ğŸ’¾ å‡ºé”™æ—¶ç´§æ€¥ä¿å­˜æ•°æ®...")
                            save_urls_to_json_batch(all_urls, output_filename, 
                                                   datetime.now().strftime("%Y-%m-%d %H:%M:%S"), 
                                                   is_final=False)
                            break
                
                except Exception as e:
                    print(f"âŒ å¤„ç†ç¬¬ {page_num + 1} é¡µæ—¶å‡ºé”™: {e}")
                    # å‡ºé”™æ—¶ä¿å­˜å½“å‰æ•°æ®
                    print("ğŸ’¾ å‡ºé”™æ—¶ç´§æ€¥ä¿å­˜æ•°æ®...")
                    save_urls_to_json_batch(all_urls, output_filename, 
                                           datetime.now().strftime("%Y-%m-%d %H:%M:%S"), 
                                           is_final=False)
                    break
            
            return all_urls
            
        except Exception as e:
            print(f"âŒ è·å–URLæ—¶å‡ºé”™: {e}")
            # å‡ºé”™æ—¶ä¿å­˜å·²è·å–çš„æ•°æ®
            if all_urls:
                print("ğŸ’¾ å¼‚å¸¸é€€å‡ºå‰ä¿å­˜å·²è·å–çš„æ•°æ®...")
                save_urls_to_json_batch(all_urls, output_filename, 
                                       datetime.now().strftime("%Y-%m-%d %H:%M:%S"), 
                                       is_final=False)
            return all_urls
        finally:
            print(f"ğŸ”š è·å–URLå®Œæˆ, å…³é—­æµè§ˆå™¨")
            browser.close()

def print_summary(urls):
    """
    æ‰“å°å¤„ç†ç»“æœæ‘˜è¦
    """
    print("\n" + "="*80)
    print("ğŸ“‹ å¤„ç†ç»“æœæ‘˜è¦")
    print("="*80)
    
    for i, url_info in enumerate(urls[-10:]):  # åªæ˜¾ç¤ºæœ€å10æ¡
        status = "âœ…" if url_info.get('final_url') != 'è·å–å¤±è´¥' else "âŒ"
        
        print(f"{status} {url_info['source']}: {url_info['name']}")
        print(f"   ğŸ“ æœ€ç»ˆURL: {url_info['final_url']}")
        print()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    target_url = "https://www.cqggzy.com/jyxx/transaction_detail.html"
    max_pages = 500  # å»ºè®®å…ˆä»å°æ•°é‡å¼€å§‹æµ‹è¯•
    batch_size = 1  # æ¯10é¡µä¿å­˜ä¸€æ¬¡
    
    print("ğŸš€ å¼€å§‹è·å–URLså¹¶è·Ÿè¸ªé‡å®šå‘...")
    print(f"ğŸ¯ ç›®æ ‡URL: {target_url}")
    print(f"ğŸ“„ å°†å¤„ç†å‰{max_pages}é¡µæ•°æ®")
    print(f"ğŸ’¾ æ¯{batch_size}é¡µä¿å­˜ä¸€æ¬¡æ•°æ®")
    print("-" * 80)
    
    timebegin = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # # è·å–æ‰€æœ‰URLåŠå…¶é‡å®šå‘ä¿¡æ¯
    # urls = get_all_popup_urls_with_redirect(
    #     target_url=target_url, 
    #     max_pages=max_pages,
    #     batch_size=batch_size,
    #     output_filename="jyxx_final_urls.json",
    #     resume=True  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
    # )
    
    # if urls:
    #     # æ‰“å°æ‘˜è¦ï¼ˆåªæ˜¾ç¤ºæœ€åå‡ æ¡ï¼‰
    #     print_summary(urls)
        
    #     # æœ€ç»ˆä¿å­˜
    #     save_success = save_urls_to_json_batch(urls, "jyxx_final_urls.json", timebegin, is_final=True)
    #     if save_success:
    #         print("ğŸ‰ æœ€ç»ˆæ•°æ®å·²æˆåŠŸä¿å­˜åˆ° jyxx_final_urls.json")
    #     else:
    #         print("ğŸ’¥ ä¿å­˜æœ€ç»ˆæ•°æ®å¤±è´¥")
    # else:
    #     print("ğŸ’¥ æ²¡æœ‰è·å–åˆ°ä»»ä½•URLæ•°æ®")
    
    print("\n" + "="*50)
    print("å¤„ç†ç¬¬äºŒä¸ªURL...")
    print("="*50)
    
    # å¤„ç†ç¬¬äºŒä¸ªURL
    target_url = "https://www.cqggzy.com/jyjg/transaction_detail.html"
    
    print("ğŸš€ å¼€å§‹è·å–URLså¹¶è·Ÿè¸ªé‡å®šå‘...")
    print(f"ğŸ¯ ç›®æ ‡URL: {target_url}")
    print(f"ğŸ“„ å°†å¤„ç†å‰{max_pages}é¡µæ•°æ®")
    print(f"ğŸ’¾ æ¯{batch_size}é¡µä¿å­˜ä¸€æ¬¡æ•°æ®")
    print("-" * 80)
    
    # è·å–æ‰€æœ‰URLåŠå…¶é‡å®šå‘ä¿¡æ¯
    urls = get_all_popup_urls_with_redirect(
        target_url=target_url, 
        max_pages=max_pages,
        batch_size=batch_size,
        output_filename="jyjg_final_urls.json",
        resume=True  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
    )
    
    if urls:
        # æ‰“å°æ‘˜è¦
        print_summary(urls)
        
        # æœ€ç»ˆä¿å­˜
        save_success = save_urls_to_json_batch(urls, "jyjg_final_urls.json", timebegin, is_final=True)
        if save_success:
            print("ğŸ‰ æœ€ç»ˆæ•°æ®å·²æˆåŠŸä¿å­˜åˆ° jyjg_final_urls.json")
        else:
            print("ğŸ’¥ ä¿å­˜æœ€ç»ˆæ•°æ®å¤±è´¥")
    else:
        print("ğŸ’¥ æ²¡æœ‰è·å–åˆ°ä»»ä½•URLæ•°æ®")
    
    print("\nğŸ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")